'''
è®­ç»ƒRNNæ¨¡å‹ä½¿å¾—  "hello" -> "ohlol"
è¾“å…¥ä¸º"hello"ï¼Œå¯è®¾ç½®å­—å…¸ e -> 0 h -> 1 l -> 2 o -> 3 helloå¯¹åº”ä¸º 10223 one-hotç¼–ç æœ‰ä¸‹é¢å¯¹åº”å…³ç³»
h   1   0100            o   3
e   0   1000            h   1
l   2   0010            l   2
l   2   0010            o   3
o   3   0001            l   2
è¾“å…¥æœ‰â€œheloâ€å››ä¸ªä¸åŒç‰¹å¾äºæ˜¯input_size = 4
hidden_size = 4 batch_size = 1

RNNæ¨¡å‹ç»´åº¦çš„ç¡®è®¤è‡³å…³é‡è¦ï¼š
rnn = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size,num_layers=num_layers)
outputs, hidden_outs = rnn(inputs, hiddens):
    inputs of shape ğ‘ ğ‘’ğ‘ğ‘†ğ‘–ğ‘§ğ‘’, ğ‘ğ‘ğ‘¡ğ‘â„, ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡_ğ‘ ğ‘–ğ‘§ğ‘’
    hiddens of shape ğ‘›ğ‘¢ğ‘šğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘ , ğ‘ğ‘ğ‘¡ğ‘â„, â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘ ğ‘–ğ‘§ğ‘’
    outputs of shape ğ‘ ğ‘’ğ‘ğ‘†ğ‘–ğ‘§ğ‘’, ğ‘ğ‘ğ‘¡ğ‘â„, â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘ ğ‘–ğ‘§ğ‘’
    hidden_outs of shape ğ‘ ğ‘’ğ‘ğ‘†ğ‘–ğ‘§ğ‘’, ğ‘ğ‘ğ‘¡ğ‘â„, â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘ ğ‘–ğ‘§ğ‘’
cell = torch.nn.RNNcell(input_size=input_size, hidden_size=hidden_size)
output, hidden_out = cell(input, hidden):
    input of shape ğ‘ğ‘ğ‘¡ğ‘â„, ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡_ğ‘ ğ‘–ğ‘§ğ‘’
    hidden of shape ğ‘ğ‘ğ‘¡ğ‘â„, â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘ ğ‘–ğ‘§ğ‘’
    output of shape ğ‘ğ‘ğ‘¡ğ‘â„, â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘ ğ‘–ğ‘§ğ‘’
    hidden_out of shape ğ‘ğ‘ğ‘¡ğ‘â„, â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘ ğ‘–ğ‘§ğ‘’
å…¶ä¸­ï¼ŒseqSizeï¼šè¾“å…¥ä¸ªæ•°  batchï¼šæ‰¹é‡å¤§å°  input_sizeï¼šç‰¹å¾ç»´æ•° numLayersï¼šç½‘ç»œå±‚æ•°  hidden_sizeï¼šéšè—å±‚ç»´æ•°
'''
import torch

idx2char = ['e', 'h', 'l', 'o']  # æ–¹ä¾¿æœ€åè¾“å‡ºç»“æœ
x_data = [1, 0, 2, 2, 3]  # è¾“å…¥å‘é‡
y_data = [3, 1, 2, 3, 2]  # æ ‡ç­¾

one_hot_lookup = [[1, 0, 0, 0],  # æŸ¥è¯¢ont hotç¼–ç  æ–¹ä¾¿è½¬æ¢
                  [0, 1, 0, 0],
                  [0, 0, 1, 0],
                  [0, 0, 0, 1]]
x_one_hot = [one_hot_lookup[x] for x in x_data]  # æŒ‰"1 0 2 2 3"é¡ºåºå–one_hot_lookupä¸­çš„å€¼èµ‹ç»™x_one_hot
'''è¿è¡Œç»“æœä¸ºx_one_hot = [ [0, 1, 0, 0],
                          [1, 0, 0, 0],
                          [0, 0, 1, 0],
                          [0, 0, 1, 0],
                          [0, 0, 0, 1] ]
åˆšå¥½å¯¹åº”è¾“å…¥å‘é‡ï¼Œä¹Ÿå¯¹åº”ç€å­—ç¬¦å€¼'hello'
'''


def cell():
    input_size = 4
    hidden_size = 4
    batch_size = 1
    inputs = torch.Tensor(x_one_hot).view(-1, batch_size, input_size)
    labels = torch.LongTensor(y_data).view(-1, 1)  # å¢åŠ ç»´åº¦æ–¹ä¾¿è®¡ç®—loss

    class cell_Model(torch.nn.Module):
        def __init__(self, input_size, hidden_size, batch_size):
            super(cell_Model, self).__init__()
            self.input_size = input_size
            self.batch_size = batch_size
            self.hidden_size = hidden_size
            self.rnncell = torch.nn.RNNCell(input_size=self.input_size, hidden_size=self.hidden_size)

        def forward(self, input, hidden):
            hidden = self.rnncell(input, hidden)  # shape: ğ‘ğ‘ğ‘¡ğ‘â„, ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡_ğ‘ ğ‘–ğ‘§ğ‘’
            return hidden

        def init_hidden(self):
            return torch.zeros(self.batch_size, self.hidden_size)  # æä¾›åˆå§‹åŒ–éšè—å±‚ï¼ˆh0ï¼‰

    net = cell_Model(input_size, hidden_size, batch_size)

    # ---è®¡ç®—æŸå¤±å’Œæ›´æ–°
    criterion = torch.nn.CrossEntropyLoss()  # äº¤å‰ç†µ
    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)
    # ---è®¡ç®—æŸå¤±å’Œæ›´æ–°

    for epoch in range(50):  # è®­ç»ƒ50æ¬¡
        loss = 0
        optimizer.zero_grad()
        hidden = net.init_hidden()
        print('Predicten string:', end='')
        for input, label in zip(inputs, labels):  # å¹¶è¡Œéå†æ•°æ®é›† ä¸€ä¸ªä¸€ä¸ªè®­ç»ƒ
            hidden = net(input, hidden)  # shape: ğ‘ğ‘ğ‘¡ğ‘â„, ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡_ğ‘ ğ‘–ğ‘§ğ‘’        ğ‘ğ‘ğ‘¡ğ‘â„, â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘ ğ‘–ğ‘§ğ‘’
            # hiddenè¾“å‡ºç»´åº¦ ğ‘ğ‘ğ‘¡ğ‘â„, â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘ ğ‘–ğ‘§ğ‘’
            loss += criterion(hidden, label)
            _, idx = hidden.max(dim=1)  # ä»ç¬¬ä¸€ä¸ªç»´åº¦ä¸Šå–å‡ºé¢„æµ‹æ¦‚ç‡æœ€å¤§çš„å€¼å’Œè¯¥å€¼æ‰€åœ¨åºå·
            print(idx2char[idx.item()], end='')  # æŒ‰ä¸Šé¢åºå·è¾“å‡ºç›¸åº”å­—æ¯å­—ç¬¦
        loss.backward()
        optimizer.step()
        print(', Epoch [%d/50] loss=%.4f' % (epoch + 1, loss.item()))


def RNN_module():
    input_size = 4
    hidden_size = 4
    num_layers = 1
    batch_size = 1
    seq_len = 5
    inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size)
    labels = torch.LongTensor(y_data)

    class RNNModel(torch.nn.Module):
        def __init__(self, input_size, hidden_size, batch_size, num_layers=1):
            super(RNNModel, self).__init__()
            self.num_layers = num_layers
            self.input_size = input_size
            self.batch_size = batch_size
            self.hidden_size = hidden_size
            self.rnn = torch.nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=num_layers)

        def forward(self, input):
            hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size)  # æä¾›åˆå§‹åŒ–éšè—å±‚ï¼ˆh0ï¼‰
            out, _ = self.rnn(input, hidden)  # out=[ h0, h1, h2, h3, h4]  _ = [[[h4]]]
            return out.view(-1, self.hidden_size)

    net = RNNModel(input_size, hidden_size, batch_size, num_layers)
    # ---è®¡ç®—æŸå¤±å’Œæ›´æ–°
    criterion = torch.nn.CrossEntropyLoss()  # äº¤å‰ç†µ
    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)
    # ---è®¡ç®—æŸå¤±å’Œæ›´æ–°

    for epoch in range(100):  # è®­ç»ƒ100æ¬¡
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        _, idx = outputs.max(dim=1)  ##ä»ç¬¬ä¸€ä¸ªç»´åº¦ä¸Šå–å‡ºé¢„æµ‹æ¦‚ç‡æœ€å¤§çš„å€¼å’Œè¯¥å€¼æ‰€åœ¨åºå·
        idx = idx.data.numpy()
        print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')
        print(', Epoch [%d/100] loss = %.3f' % (epoch + 1, loss.item()))


def embedding():
    num_class = 4  # ç±»åˆ«æ•°é‡
    input_size = 4  # è¾“å…¥ç»´åº¦
    hidden_size = 8  # éšè—å±‚ç»´åº¦
    embedding_size = 10  # åµŒå…¥åˆ°10ç»´ç©ºé—´
    num_layers = 2  # RNNå±‚æ•°
    batch_size = 1
    seq_len = 5  # æ•°æ®é‡
    idx2char = ['e', 'h', 'l', 'o']  # æ–¹ä¾¿æœ€åè¾“å‡ºç»“æœ
    x_data = [[1, 0, 2, 2, 3]]  # (batch, seq_len)
    y_data = [3, 1, 2, 3, 2]  # (batch * seq_len)

    inputs = torch.LongTensor(x_data)
    labels = torch.LongTensor(y_data)

    class Model(torch.nn.Module):
        def __init__(self):
            super(Model, self).__init__()
            self.emb = torch.nn.Embedding(input_size, embedding_size)
            self.rnn = torch.nn.RNN(input_size=embedding_size,
                                    hidden_size=hidden_size,
                                    num_layers=num_layers,
                                    batch_first=True)
            self.fc = torch.nn.Linear(hidden_size, num_class)

        def forward(self, x):
            hidden = torch.zeros(num_layers, x.size(0), hidden_size)
            x = self.emb(x)  # input (batch,seqLen)  output (batch, seqLen, embeddingSize)
            x, _ = self.rnn(x, hidden)
            x = self.fc(x)
            return x.view(-1, num_class)  # ä¿®æ”¹ç»´åº¦å¥½ç”¨äº¤å‰ç†µè®¡ç®—æŸå¤±

    net = Model()
    # ---è®¡ç®—æŸå¤±å’Œæ›´æ–°
    criterion = torch.nn.CrossEntropyLoss()  # äº¤å‰ç†µ
    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)
    # ---è®¡ç®—æŸå¤±å’Œæ›´æ–°

    for epoch in range(15):  # è®­ç»ƒ15æ¬¡
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        _, idx = outputs.max(dim=1)  ##ä»ç¬¬ä¸€ä¸ªç»´åº¦ä¸Šå–å‡ºé¢„æµ‹æ¦‚ç‡æœ€å¤§çš„å€¼å’Œè¯¥å€¼æ‰€åœ¨åºå·
        idx = idx.data.numpy()
        print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')
        print(', Epoch [%d/15] loss = %.3f' % (epoch + 1, loss.item()))


if __name__ == '__main__':
    # cell()
    # RNN_module()
    embedding()
